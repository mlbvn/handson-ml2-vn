{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phụ lục D – Autodiff**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Noteboook này chứa các khai triển ví dụ của các kỹ thuật autodiff khác nhau để giải thích cách mà chúng hoạt động._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/mlbvn/handson-ml2-vn/blob/main/extra_autodiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/mlbvn/handson-ml2-vn/blob/main/extra_autodiff.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cài đặt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Giới thiệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giả sử ta cần tính gradient của hàm  $f(x,y)=x^2y + y + 2$ theo tham số x và y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    return x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một phương pháp để giải bài toán một cách phân tích:\n",
    "\n",
    "$\\dfrac{\\partial f}{\\partial x} = 2xy$\n",
    "\n",
    "$\\dfrac{\\partial f}{\\partial y} = x^2 + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x,y):\n",
    "    return 2*x*y, x*x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ $\\dfrac{\\partial f}{\\partial x}(3,4) = 24$ và $\\dfrac{\\partial f}{\\partial y}(3,4) = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoàn hảo! Ta cũng có thể tìm phương trình cho đạo hàm bậc hai (còn được gọi là Hessian):\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial x \\partial x} = \\dfrac{\\partial (2xy)}{\\partial x} = 2y$\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial x \\partial y} = \\dfrac{\\partial (2xy)}{\\partial y} = 2x$\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial y \\partial x} = \\dfrac{\\partial (x^2 + 1)}{\\partial x} = 2x$\n",
    "\n",
    "$\\dfrac{\\partial^2 f}{\\partial y \\partial y} = \\dfrac{\\partial (x^2 + 1)}{\\partial y} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tại x=3 và y=4, các Hessian lần lượt là 8, 6, 6, 0. Hãy sử dụng các phương trình trên để tính chúng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2f(x, y):\n",
    "    return [2*y, 2*x], [2*x, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([8, 6], [6, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2f(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuyệt, nhưng ta cần phải làm một số toán. Không quá khó trong trường hợp này nhưng cho mạng nơ-ron sâu, trên thực tế không thể tính đạo hàm bằng cách này. Vì vậy hãy xem một vài cách để tự động hoá tính toán!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tính vi phân số học"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở đây, ta tính xấp xỉ gradient sử dụng phương trình: $\\dfrac{\\partial f}{\\partial x} = \\displaystyle{\\lim_{\\epsilon \\to 0}}\\dfrac{f(x+\\epsilon, y) - f(x, y)}{\\epsilon}$ (và có một định nghĩa tương tự cho  $\\dfrac{\\partial f}{\\partial y}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(func, vars_list, eps=0.0001):\n",
    "    partial_derivatives = []\n",
    "    base_func_eval = func(*vars_list)\n",
    "    for idx in range(len(vars_list)):\n",
    "        tweaked_vars = vars_list[:]\n",
    "        tweaked_vars[idx] += eps\n",
    "        tweaked_func_eval = func(*tweaked_vars)\n",
    "        derivative = (tweaked_func_eval - base_func_eval) / eps\n",
    "        partial_derivatives.append(derivative)\n",
    "    return partial_derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x, y):\n",
    "    return gradients(f, [x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24.000400000048216, 10.000000000047748]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nó hoạt động tốt!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tin tốt là tính Hessian khá dễ. Đầu tiên hãy tạo các hàm tính đạo hàm bậc một (còn gọi là Jacobian):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.000400000048216, 10.000000000047748)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dfdx(x, y):\n",
    "    return gradients(f, [x,y])[0]\n",
    "\n",
    "def dfdy(x, y):\n",
    "    return gradients(f, [x,y])[1]\n",
    "\n",
    "dfdx(3., 4.), dfdy(3., 4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ ta có thể đơn giản áp dụng hàm  `gradients()` vào các hàm này:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2f(x, y):\n",
    "    return [gradients(dfdx, [x, y]), gradients(dfdy, [x, y])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7.999999951380232, 6.000099261882497],\n",
       " [6.000099261882497, -1.4210854715202004e-06]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2f(3, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vậy là mọi thứ hoạt động tốt, nhưng kết quả được xấp xỉ, và tính toán gradient của một hàm theo $n$ biến cần gọi hàm đó $n$ lần. Trong các mạng nơ-ron sâu, thường có hàng ngàn tham số để tinh chỉnh sử dụng phương pháp hạ gradient (điều này cần tính toán gradient của hàm mất mát theo mỗi tham số), vì vậy phương pháp này sẽ rất chậm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triển khai một Đồ thị Tính toán Giả lập"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thay vì phương pháp số học này, hãy triển khai một vài phương pháp autodiff biểu trưng. Để làm điều này, ta cần định nghĩa các lớp để miêu tả hằng số, biến số và các toán tử. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, name, init_value=0):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() + self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() * self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tốt, bây giờ ta có thể xây dựng một đồ thị tính toán để miêu tả hàm $f$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Var(\"x\")\n",
    "y = Var(\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Và ta có thể chạy đồ thị này để tính $f$ tại bất cứ điểm nào, ví dụ $f(3, 4)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value = 3\n",
    "y.value = 4\n",
    "f.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoàn hảo, ta đã tìm thấy kết quả cuối cùng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tính toán gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các phương pháp autodiff ta sẽ trình bày dưới đây đều dựa trên *quy tắc dây chuyền*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giả sử ta có hai hàm $u$ và $v$, và ta áp dụng chúng tuần tự cho một số đầu vào $x$, và ta thu được kết quả $z$. Vì thế ta có  $z = v(u(x))$ mà ta có thể viết lại thành  $z = v(s)$ và $s = u(x)$. Bây giờ ta có thể áp dụng quy tắc dây chuyền để thu được đạo hàm riêng của đầu ra $z$ theo đầu vào $x$:\n",
    "\n",
    "$ \\dfrac{\\partial z}{\\partial x} = \\dfrac{\\partial s}{\\partial x} \\cdot \\dfrac{\\partial z}{\\partial s}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ nếu $z$ là đầu ra của chuỗi các hàm có các đầu ra trung gian $s_1, s_2, ..., s_n$, quy tắc dây chuyền vẫn được áp dụng:\n",
    "\n",
    "$ \\dfrac{\\partial z}{\\partial x} = \\dfrac{\\partial s_1}{\\partial x} \\cdot \\dfrac{\\partial s_2}{\\partial s_1} \\cdot \\dfrac{\\partial s_3}{\\partial s_2} \\cdot \\dots \\cdot \\dfrac{\\partial s_{n-1}}{\\partial s_{n-2}} \\cdot \\dfrac{\\partial s_n}{\\partial s_{n-1}} \\cdot \\dfrac{\\partial z}{\\partial s_n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong autodiff thuận, thuật toán tính các số hạng này theo \"chiều thuận\" (nghĩa là cùng thứ tự với tính toán cần để tính đầu ra $z$), từ trái qua phải: đầu tiên $\\dfrac{\\partial s_1}{\\partial x}$, tiếp theo  $\\dfrac{\\partial s_2}{\\partial s_1}$, v.v. Trong autodiff ngược, thuật toán tính các số hạng này theo \"chiều ngược\", từ phải qua trái: đầu tiên $\\dfrac{\\partial z}{\\partial s_n}$, tiếp đó $\\dfrac{\\partial s_n}{\\partial s_{n-1}}$, v.v.\n",
    "\n",
    "Ví dụ, giả sử ta muốn tính đạo hàm của hàm  $z(x)=\\sin(x^2)$ tại x=3, sử dụng autodiff thuận. Thuật toán đầu tiên sẽ tính đạo hàm riêng $\\dfrac{\\partial s_1}{\\partial x}=\\dfrac{\\partial x^2}{\\partial x}=2x=6$. Tiếp theo, nó sẽ tính $\\dfrac{\\partial z}{\\partial x}=\\dfrac{\\partial s_1}{\\partial x}\\cdot\\dfrac{\\partial z}{\\partial s_1}= 6 \\cdot \\dfrac{\\partial \\sin(s_1)}{\\partial s_1}=6 \\cdot \\cos(s_1)=6 \\cdot \\cos(3^2)\\approx-5.46$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy kiểm tra kết quả này sử dụng hàm `gradients()` được định nghĩa ở trên:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5.46761419430053]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sin\n",
    "\n",
    "def z(x):\n",
    "    return sin(x**2)\n",
    "\n",
    "gradients(z, [3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trông có vẻ ổn. Bây giờ hãy làm điều tương tự bằng cách sử dụng autodiff ngược. Lần này thuật toán sẽ bắt đầu từ bên phải vì thế nó sẽ tính $\\dfrac{\\partial z}{\\partial s_1} = \\dfrac{\\partial \\sin(s_1)}{\\partial s_1}=\\cos(s_1)=\\cos(3^2)\\approx -0.91$. Tiếp theo nó sẽ tính $\\dfrac{\\partial z}{\\partial x}=\\dfrac{\\partial s_1}{\\partial x}\\cdot\\dfrac{\\partial z}{\\partial s_1} \\approx \\dfrac{\\partial s_1}{\\partial x} \\cdot -0.91 = \\dfrac{\\partial x^2}{\\partial x} \\cdot -0.91=2x \\cdot -0.91 = 6\\cdot-0.91=-5.46$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tất nhiên cả hai phương pháp đều cho kết quả như nhau (ngoại trừ lỗi làm tròn), và với một đầu vào và đầu ra duy nhất, chúng cần cùng một số lượng phép tính. Nhưng khi có một vài đầu vào và đầu ra, chúng có thể có hoạt động rất khác nhau. Thật vậy, nếu có nhiều đầu vào, số hạng ở ngoài cùng bên phải sẽ cần được tính đạo hàm riêng theo mỗi đầu vào, do đó, nên tính các số hạng ngoài cùng bên phải trước. Điều này tương đương với việc sử dụng autodiff ngược. \n",
    "\n",
    "Bằng cách này, các số hạng ngoài cùng bên phải có thể được tính chỉ một lần và được sử dụng để tính tất cả các đạo hàm riêng. Ngược lại, nếu có nhiều đầu ra, autodiff thuận thường được ưu tiên hơn vì các số hạng ngoài cùng bên trái có thể được tính toán chỉ một lần để tính các đạo hàm riêng của các đầu ra khác nhau. Trong Học Sâu, thường có hàng ngàn tham số mô hình, nghĩa là có rất nhiều đầu vào nhưng ít đầu ra. Trên thực tế, nhìn chung chỉ có một đầu ra trong suốt quá trình huấn luyện: giá trị mất mát. Đây là lý do vì sao autodiff ngược được sử dụng trong TensorFlow và tất cả các thư viện Học Sâu chính."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có một số phức tạp bổ sung trong autodiff ngược: giá trị của $s_i$ thường cần để tính $\\dfrac{\\partial s_{i+1}}{\\partial s_i}$, và tính $s_i$ cần tính $s_(i-1)$ trước, nghĩa là cần tính  $s_{i-2}$, v.v. Vì vậy về cơ bản, một vòng tính toán thuận xuyên suốt mạng cần thiết để tính  $s_1$, $s_2$, $s_3$, $\\dots$, $s_{n-1}$ và $s_n$, và tiếp theo thuật toán có thể tính đạo hàm riêng từ phải qua trái. Lưu tất cả các giá trị trung gian $s_i$ trong RAM thường là một vấn đề, đặc biệt khi xử lý ảnh, và khi sử dụng GPU thường có dung lượng RAM hạn chế: để hạn chế vấn đề này, ta có thể giảm số lượng tầng trong mạng nơ-ron, hoặc cài đặt TensorFlow để hoán đổi các giá trị này từ RAM GPU sang RAM CPU. Một cách khác là chỉ lưu một số giá trị trung gian ở bước lẻ $s_1$, $s_3$, $s_5$, $\\dots$, $s_{n-4}$, $s_{n-2}$ và $s_n$. Điều này nghĩa là khi thuật toán tính đạo hàm riêng, nếu một giá trị trung gian $s_i$ bị thiếu, nó sẽ tính lại dựa trên giá trị trung gian trước đó $s_(i-1)$. Việc này đánh đổi CPU với RAM (nếu bạn muốn tìm hiểu thêm, xem [bài báo này](https://pdfs.semanticscholar.org/f61e/9fd5a4878e1493f7a6b03774a61c17b7e9a4.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff thuận"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Const.gradient = lambda self, var: Const(0)\n",
    "Var.gradient = lambda self, var: Const(1) if self is var else Const(0)\n",
    "Add.gradient = lambda self, var: Add(self.a.gradient(var), self.b.gradient(var))\n",
    "Mul.gradient = lambda self, var: Add(Mul(self.a, self.b.gradient(var)), Mul(self.a.gradient(var), self.b))\n",
    "\n",
    "x = Var(name=\"x\", init_value=3.)\n",
    "y = Var(name=\"y\", init_value=4.)\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2\n",
    "\n",
    "dfdx = f.gradient(x)  # 2xy\n",
    "dfdy = f.gradient(y)  # x² + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.0, 10.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfdx.evaluate(), dfdy.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vì đầu ra của phương thức `gradient()` thì hoàn toàn biểu trưng, ta không bị giới hạn ở việc chỉ tính đạo hàm bậc một, ta cũng có thể tính đạo hàm bậc hai, v.v.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2fdxdx = dfdx.gradient(x) # 2y\n",
    "d2fdxdy = dfdx.gradient(y) # 2x\n",
    "d2fdydx = dfdy.gradient(x) # 2x\n",
    "d2fdydy = dfdy.gradient(y) # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8.0, 6.0], [6.0, 0.0]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[d2fdxdx.evaluate(), d2fdxdy.evaluate()],\n",
    " [d2fdydx.evaluate(), d2fdydy.evaluate()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý rằng kết quả bây giờ là chính xác, không phải là xấp xỉ (tất nhiên là đến giới hạn độ chính xác số động của máy tính)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff thuận sử dụng các số đối ngẫu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một cách hay để áp dụng autodiff thuận là sử dụng [các số đối ngẫu](https://en.wikipedia.org/wiki/Dual_number). Nói một cách ngắn gọn, một số đối ngẫu $z$ có dạng  $z = a + b\\epsilon$, trong đó $a$ và $b$ là các số thực, và  $\\epsilon$ là một số cực nhỏ, dương nhưng nhỏ hơn tất cả các số thực, và sao cho  $\\epsilon^2=0$.\n",
    "Ta có thể chứng minh  $f(x + \\epsilon) = f(x) + \\dfrac{\\partial f}{\\partial x}\\epsilon$, vì thế đơn giản bằng cách tính $f(x + \\epsilon)$ ta có thể thu được cả giá trị của $f(x)$ và đạo hàm riên của $f$ theo $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các số đối ngẫu có quy tắc số học riêng của chúng, thường khá là tự nhiên. Ví dụ:\n",
    "\n",
    "**Phép cộng**\n",
    "\n",
    "$(a_1 + b_1\\epsilon) + (a_2 + b_2\\epsilon) = (a_1 + a_2) + (b_1 + b_2)\\epsilon$\n",
    "\n",
    "**Phép trừ**\n",
    "\n",
    "$(a_1 + b_1\\epsilon) - (a_2 + b_2\\epsilon) = (a_1 - a_2) + (b_1 - b_2)\\epsilon$\n",
    "\n",
    "**Phép nhân**\n",
    "\n",
    "$(a_1 + b_1\\epsilon) \\times (a_2 + b_2\\epsilon) = (a_1 a_2) + (a_1 b_2 + a_2 b_1)\\epsilon + b_1 b_2\\epsilon^2 = (a_1 a_2) + (a_1b_2 + a_2b_1)\\epsilon$\n",
    "\n",
    "**Phép chia**\n",
    "\n",
    "$\\dfrac{a_1 + b_1\\epsilon}{a_2 + b_2\\epsilon} = \\dfrac{a_1 + b_1\\epsilon}{a_2 + b_2\\epsilon} \\cdot \\dfrac{a_2 - b_2\\epsilon}{a_2 - b_2\\epsilon} = \\dfrac{a_1 a_2 + (b_1 a_2 - a_1 b_2)\\epsilon - b_1 b_2\\epsilon^2}{{a_2}^2 + (a_2 b_2 - a_2 b_2)\\epsilon - {b_2}^2\\epsilon} = \\dfrac{a_1}{a_2} + \\dfrac{a_1 b_2 - b_1 a_2}{{a_2}^2}\\epsilon$\n",
    "\n",
    "**Luỹ thừa**\n",
    "\n",
    "$(a + b\\epsilon)^n = a^n + (n a^{n-1}b)\\epsilon$\n",
    "\n",
    "v.v."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy tạo một lớp để mô tả các số đối ngẫu, và triển khai một số toán tử (cộng và nhân). Bạn có thể thử thêm nhiều hơn nếu bạn muốn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualNumber(object):\n",
    "    def __init__(self, value=0.0, eps=0.0):\n",
    "        self.value = value\n",
    "        self.eps = eps\n",
    "    def __add__(self, b):\n",
    "        return DualNumber(self.value + self.to_dual(b).value,\n",
    "                          self.eps + self.to_dual(b).eps)\n",
    "    def __radd__(self, a):\n",
    "        return self.to_dual(a).__add__(self)\n",
    "    def __mul__(self, b):\n",
    "        return DualNumber(self.value * self.to_dual(b).value,\n",
    "                          self.eps * self.to_dual(b).value + self.value * self.to_dual(b).eps)\n",
    "    def __rmul__(self, a):\n",
    "        return self.to_dual(a).__mul__(self)\n",
    "    def __str__(self):\n",
    "        if self.eps:\n",
    "            return \"{:.1f} + {:.1f}ε\".format(self.value, self.eps)\n",
    "        else:\n",
    "            return \"{:.1f}\".format(self.value)\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    @classmethod\n",
    "    def to_dual(cls, n):\n",
    "        if hasattr(n, \"value\"):\n",
    "            return n\n",
    "        else:\n",
    "            return cls(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3 + (3 + 4 \\epsilon) = 6 + 4\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0 + 4.0ε"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 + DualNumber(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(3 + 4ε)\\times(5 + 7ε)$ = $3 \\times 5 + 3 \\times 7ε + 4ε \\times 5 + 4ε \\times 7ε$ = $15 + 21ε + 20ε + 28ε^2$ = $15 + 41ε + 28 \\times 0$ = $15 + 41ε$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0 + 41.0ε"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DualNumber(3, 4) * DualNumber(5, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ hãy xem liệu các số đối ngẫu hoạt động với khung tính toán giả lập không:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.value = DualNumber(3.0)\n",
    "y.value = DualNumber(4.0)\n",
    "\n",
    "f.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vâng, chắc chắn hoạt động. Bây giờ hãy sử dụng cái này để tính các đạo hàm riêng của $f$ theo $x$ và $y$ tại x=3 và y=4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.value = DualNumber(3.0, 1.0)  # 3 + ε\n",
    "y.value = DualNumber(4.0)       # 4\n",
    "\n",
    "dfdx = f.evaluate().eps\n",
    "\n",
    "x.value = DualNumber(3.0)       # 3\n",
    "y.value = DualNumber(4.0, 1.0)  # 4 + ε\n",
    "\n",
    "dfdy = f.evaluate().eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfdy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuyệt! Tuy nhiên, trong triển khai này ta bị giới hạn với các đạo hàm bậc một. Bây giờ hãy xem autodiff ngược."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff ngược"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy viết lại khung giả lập để thêm autodiff ngược:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        pass\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, name, init_value=0):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "        self.gradient = 0\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.gradient += gradient\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() + self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient)\n",
    "        self.b.backpropagate(gradient)\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() * self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient * self.b.value)\n",
    "        self.b.backpropagate(gradient * self.a.value)\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Var(\"x\", init_value=3)\n",
    "y = Var(\"y\", init_value=4)\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = x²y + y + 2\n",
    "\n",
    "result = f.evaluate()\n",
    "f.backpropagate(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((x) * (x)) * (y) + y + 2\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một lần nữa, trong triển khai này các đầu ra chỉ là số, không phải là biểu thức biểu trưng, vì vậy ta chỉ tính đến đạo hàm bậc một. Tuy nhiên, ta cũng có các phương thức `backpropagate()` trả về biểu thức biểu trưng thay vì giá trị (nghĩa là trả về `Add(2,3)` thay vì 5). Điều này sẽ giúp tính toán gradient bậc hai (và hơn nữa). Đây là điều mà TensorFlow làm, cũng như là tất cả các thư viện chính triển khai autodiff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff ngược sử dụng TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=24.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(3.)\n",
    "y = tf.Variable(4.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    f = x*x*y + y + 2\n",
    "\n",
    "jacobians = tape.gradient(f, [x, y])\n",
    "jacobians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vì mọi thứ đều biểu trưng, ta có thể tính đạo hàm bậc hai, và hơn thế nữa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[<tf.Tensor: shape=(), dtype=float32, numpy=8.0>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=6.0>],\n",
       " [<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, None]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable(3.)\n",
    "y = tf.Variable(4.)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    f = x*x*y + y + 2\n",
    "    df_dx, df_dy = tape.gradient(f, [x, y])\n",
    "\n",
    "d2f_d2x, d2f_dydx = tape.gradient(df_dx, [x, y])\n",
    "d2f_dxdy, d2f_d2y = tape.gradient(df_dy, [x, y])\n",
    "del tape\n",
    "\n",
    "hessians = [[d2f_d2x, d2f_dydx], [d2f_dxdy, d2f_d2y]]\n",
    "hessians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý rằng khi tính đạo hàm của một tensor theo một biến mà nó không phụ thuộc, thay vì trả về 0.0, hàm  `gradient()` trả về `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Và đến đây là hết rồi tất cả các bạn! Hy vọng mọi người thích notebook này."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "nav_menu": {
   "height": "603px",
   "width": "616px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "f22a20af907fde35ff19e1e892fdb271353fb19b11c7ebd774491472e685293c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
